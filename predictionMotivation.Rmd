---
title: "Practical Machine Learning - Prediction Assignment"
author: "Vishanta Rayamajhi"
date: "November 21, 2015"
output: 
  html_document: 
    keep_md: yes
---

##Background
As part of the course project for **Practical Machine Learning** in Coursera, we were provided with a training and a testing set of activity data of six young health participants, collected from accelerometers on the belt, forearm, arm, and dumbell, where each participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

##Goal
The goal of the project is to predict the manner in which the participants did the exercise. Build a predictive model to be able to predict the outcome 'classe' in the testing set that falls in one of the following classes:

- Exactly according to the specification (Class A)
- Throwing the elbows to the front (Class B)
- Lifting the dumbbell only halfway (Class C)
- Lowering the dumbbell only halfway (Class D)
- Throwing the hips to the front (Class E)

##Loading data
The data is downloaded and stored in the 'data' directory. We load the data by changing the missing values those that are coded as string "#DIV/0!" or "" to "NA" in order to maintain uniformity with missing values.
```{r}
# Load training set
pml_training <- read.csv("data/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))

# Load testing set
pml_testing <- read.csv("data/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
```

##Load library
```{r}
library(caret)
library(doParallel)
```

##Data structure
```{r}
dim(pml_training)
dim(pml_testing)

table(pml_training$user_name)
table(pml_training$classe)
table(pml_training$classe, pml_training$user_name)
```

##Cleaning Data
The preliminary study of the summary of the training set reveals more insight of the structure of the data which enables us to come up with a number of cleaning actions below:

- Remove not relevant columns for classification (the first 7 columns)
- Remove columns with over 95% of NAs
- Exclude near zero variance predictors
- Convert dependent variable "classe" into factor, if not

Let's first check the columns with NA values before proceeding with cleaning the data.

```{r}
summary(colSums(is.na(pml_training)) == 0)
```

The result shows we have 100 columns with zero variability, which could be excluded in our prediction model.

```{r}
# Remove the first 7 columns
training.set <- pml_training[,-c(1:7)] ## 153 covariants

# Remove missing data
NAs <- apply(training.set, 2, function(x) {sum(is.na(x))})
training.set <- training.set[,which(NAs < nrow(training.set)*0.95)] ## 53 covariants

# Exclude near zero variance predictors
table(nearZeroVar(training.set, saveMetrics = TRUE)$nzv) # FALSE=53, implies no nzv due to data cleaning
nzv.columns <- nearZeroVar(training.set, saveMetrics = TRUE)
training.set <- training.set[,nzv.columns$nzv == FALSE] # takes the 53 'FALSE' columns

# Convert classe into factor
training.set$classe <- factor(training.set$classe)
```

##Feature Selection
The final set of features used for classification are as follows.
```{r}
names(training.set)
```

##Exploratory Data Analysis (EDA)
```{r, fig.width=5, fig.height=4}
plot(training.set$classe,
     col="cyan",
     main="Activity Summary in Training Set",
     xlab="classe levels",
     ylab="Frequency"
)
```

##Cross validation: Splitting data
Split the dataset into 60% training and 40% validation set in order to perform cross validation. This is performed using random sampling without replacement.

```{r}
inTrain <- createDataPartition(y=training.set$classe, p=0.60, list=FALSE)
training <- training.set[inTrain,]
validation <- training.set[-inTrain,]
```

##Fit a Model
We will be using a couple of model and find out the one that provides high accuracy and low out of sample error. The Principal Component Analysis (PCA) can also be added as a preprocess option of the train function, but at the expense of losing accuracy. Hence, we will omit PCA model.

```{r}
# set a seed
set.seed(6789)
```

**Random Forest**
When I first ran the Random Forest model with default train() function, it seemed to take hours to complete the run. I stopped the exeuction in the middle. The first thing to note is that by default {caret}train() uses "Resampling: Bootstraped" which is very computationally intensive.

1. Hence, I used optimization method using trControl parameter for tuning model training. And switched to cross-validation (Resampling: Cross-Validated) with a low k=3, and played around with it to get a compromise between accuracy and speed. With this the model completed in approx 6 mins.

2. In the next run, I used parallel computing for multi-core using registerDoParallel(). The 4-core CPU were fully utilized and it completed in less than 3 mins.

**Tuning with cross validation**
The model is tuned with cross validation resampling (Resampling: Cross-Validated) with 3 folds. Parallel computing method is also applied to improve efficiency.

```{r cache=TRUE}
# parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))

# train a model - random forest (rf)
modelRF <- train(classe ~ ., data=training, method="rf", trControl = trainControl(method = "cv", number = 3))
modelRF
modelRF$finalModel

# prediction
predictions <- predict(modelRF, newdata = validation)

# quick checks
table(validation$classe)
table(predictions)

# confusion matrix
confusionMatrix(predictions, validation$classe)
```

Next, to improve the model and avoid over-fitting, the cross validation technique is applied with 10 folds. This took around 11m to execute.

```{r cache=TRUE}
registerDoParallel(makeCluster(detectCores()))
modelRF10 <- train(classe ~ ., data=training, method="rf", trControl = trainControl(method = "cv", number = 10))
predictions10 <- predict(modelRF10, newdata = validation)
confusionMatrix(predictions10, validation$classe)
```

##Accuracy and Out-of-Sample Error
With k=3, the cross validation accuracy is 99.40% and the out-of-sample error is 0.60%. This is a very good model.
With k=10, the cross validation accuracy is 99.41%, therefore out-of-sample erro is 0.59. Hence, there is only a slight difference in the accuracy level.

##Variables importance
Following code snippet computes the importance of predictors and *roll_belt* is the most important covariant for the model trained with random forest and tuned with cross validation.

```{r}
varImp(modelRF$finalModel)
```

Let's visualize this in a plot.
```{r, fig.width=5, fig.height=4}
plot(varImp(modelRF), top = 20)
```

##Predicting 20 test cases
```{r}
predictions_test <- predict(modelRF, newdata = pml_testing)
predictions_test
```

##Conclusion
Random Forest model gives prediction with highest accuracy and lowest out of sample error and when tuned with cross validation imporves efficiency. It provides better accuracy as compared to Decision Tree, Boosted Tree and Linear Discriminant Analysis models. Due to report size constraint, the other prediction models could not be performed. Few reasons to pick the model are:

- Random forest is most suitable at handling a large number of inputs, especially when the interactions between variables are unknown.
- Random forest’s built in cross-validation component that gives an unbiased estimate of the forest’s out-of-sample (or bag) (OOB) error rate.
- Random forest can handle unscaled variables and categorical variables that makes it bit easy in the data cleaning process.